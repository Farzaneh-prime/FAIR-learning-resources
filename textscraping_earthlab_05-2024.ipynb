{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EduTrain curriculum\n",
    "(enviroment: FAIR-OER python3.8.18)\n",
    "\n",
    "Following imports make the functionalities of rest of the code possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import random\n",
    "import pdftotext\n",
    "import pandas as pd\n",
    "import ast\n",
    "from googletrans import Translator\n",
    "from langdetect import detect, LangDetectException\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import rdflib\n",
    "from rdflib import Namespace, Graph, URIRef, Literal, XSD, RDF, RDFS, OWL, DCTERMS\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathes\n",
    "eo4geo_json = r\"B:\\LMS\\ETKG\\Data\\EO4GEO\\EO4GEO - cleaned.json\"\n",
    "eo4geo_csv = r\"B:\\LMS\\ETKG\\Data\\EO4GEO\\EO4GEO - cleaned.csv\"\n",
    "earthlab_dir = r\"B:\\LMS\\Courses\\earthlab\"\n",
    "earthlab_csv = r\"B:\\LMS\\ETKG\\Data\\EarthLab\\earthlab.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "Data sources are:\n",
    "1. EO4GEO\n",
    "2. EarthLab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EO4GEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(eo4geo_json, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "rows = []\n",
    "for key, entry in data.items():\n",
    "    if 'relations' in entry:\n",
    "        for relation in entry['relations']:\n",
    "            row = {\n",
    "                'id': entry.get('id', ''),\n",
    "                'name': entry.get('name', ''),\n",
    "                'relation_name': relation.get('name', ''),\n",
    "                'relation_source': relation.get('source', ''),\n",
    "                'relation_target': relation.get('target', ''),\n",
    "                'skills': \", \".join(entry.get('skills', [])),\n",
    "                'uri': entry.get('uri', '')\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(eo4geo_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EarthLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def earthLab(md_path):\n",
    "    with open(md_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        parts = content.split('---')\n",
    "        metadata = yaml.load(parts[1], Loader=yaml.FullLoader)\n",
    "        description = content.split(\"<div class=\")[1].split(\"</div>\")[0].strip()\n",
    "        if \"What You Need\" in content:\n",
    "            prerequisites = content.split(\"What You Need\")[1].split(\"</div>\")[0].strip()\n",
    "        else:\n",
    "            prerequisites = \"none\"\n",
    "        metadata['description'] = description\n",
    "        metadata['prerequisites'] = prerequisites         \n",
    "        return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(directory_path, csv_path):\n",
    "    md_files = Path(directory_path).rglob('*.md')\n",
    "    all_metadata = []\n",
    "    \n",
    "    for md_file in md_files:\n",
    "        try:\n",
    "            md_data = earthLab(md_file)\n",
    "            if md_data:\n",
    "                all_metadata.append(md_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {md_file}: This is not an educational file. Error: {e}\")\n",
    "\n",
    "    if not all_metadata:\n",
    "        print(\"No valid Markdown files processed.\")\n",
    "        return\n",
    "\n",
    "    fieldnames = set()\n",
    "    for metadata in all_metadata:\n",
    "        fieldnames.update(metadata.keys())\n",
    "\n",
    "    fieldnames = sorted(fieldnames)\n",
    "\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for metadata in all_metadata:\n",
    "            row = {key: json.dumps(value) if isinstance(value, dict) else value for key, value in metadata.items()}\n",
    "            writer.writerow(row)\n",
    "    return all_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(earthlab_dir, earthlab_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdftocsv(pdf_path, start=1, end=None, header=0, footer=0, keywords={}):\n",
    "    # Paths\n",
    "    text_path = pdf_path.replace('.pdf', '.txt')\n",
    "    csv_path = pdf_path.replace('.pdf', '.csv')\n",
    "    # Read PDF\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf = pdftotext.PDF(pdf_file)\n",
    "        end = end or len(pdf)\n",
    "    # Write relevant PDF content to a text file\n",
    "    with open(text_path, 'w', encoding='utf-8') as pdf_content:\n",
    "        for page_number in range(start - 1, end):\n",
    "            lines = pdf[page_number].splitlines()\n",
    "            main_lines = lines[header:-footer or None]\n",
    "            main_content = '\\n'.join(main_lines)\n",
    "            pdf_content.write(main_content + '\\n')\n",
    "    # Extract course information and save it as CSV\n",
    "    with open(text_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        data = {}\n",
    "        max_length = 0\n",
    "        for keyword_one, keyword_two in keywords.items():\n",
    "            sections = content.split(keyword_one)[1:]\n",
    "            matches = [section.split(keyword_two)[0] for section in sections]\n",
    "            data[keyword_one] = matches\n",
    "            max_length = max(max_length, len(matches))\n",
    "        for key in data:\n",
    "            while len(data[key]) < max_length:\n",
    "                data[key].append('N/A')\n",
    "        df = pd.DataFrame(data)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary = pd.read_csv(r\"B:\\Publications\\FAIR OER\\02-Data\\00-Raw\\data_summary.csv\")\n",
    "for index, row in data_summary.iterrows():\n",
    "    pdf_path = row['pdf_path']\n",
    "    start = row['start']\n",
    "    end = row['end']\n",
    "    header = row['header']\n",
    "    footer = row['footer']\n",
    "    keywords = ast.literal_eval(row['keywords'])\n",
    "    pdftocsv(pdf_path, start, end, header, footer, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = pd.concat([pd.read_csv(pdf_path.replace('.pdf', '.csv')) for pdf_path in data_summary['pdf_path']], ignore_index=True)\n",
    "courses.to_csv(r\"B:\\Publications\\FAIR OER\\02-Data\\01-Processed\\courses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DEtoEN(csv_path):\n",
    "    output_path = csv_path.replace('.csv', '_en.csv')\n",
    "    df = pd.read_csv(csv_path)\n",
    "    translator = Translator()\n",
    "    for i in range(len(df)):\n",
    "        for j in range(len(df.columns)):\n",
    "            text = df.iat[i, j]\n",
    "            if pd.notna(text):\n",
    "                try:\n",
    "                    lang = detect(text)\n",
    "                except LangDetectException:\n",
    "                    print(f\"Cannot detect language for text: {text[:30]}...\")\n",
    "                    lang = None\n",
    "                if lang == 'de':\n",
    "                    df.iat[i, j] = translator.translate(text, src='de', dest='en').text\n",
    "                else:\n",
    "                    df.iat[i, j] = text\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEtoEN(r'B:\\Publications\\FAIR OER\\02-Data\\01-Processed\\courses_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(csv_path):\n",
    "    normalized_path = csv_path.replace('.csv', '_norm.csv')\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    df = df.applymap(lambda s: s.replace('\\n', ' ').replace('\\t', ' ') if type(s) == str else s)\n",
    "    df = df.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "    df = df.replace({r'[^\\w\\s]':''}, regex=True)\n",
    "\n",
    "    df.to_csv(normalized_path, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization(r'B:\\Publications\\FAIR OER\\02-Data\\01-Processed\\courses_clean_en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns(csv_path, column_list, prerequisite_column):\n",
    "    merged_path = csv_path.replace('.csv', '_merged.csv')\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df_merged = pd.DataFrame()\n",
    "    df_merged['course'] = df.iloc[:, column_list].astype(str).apply('\\n'.join, axis=1)\n",
    "    df_merged['prerequisites'] = df.iloc[:, prerequisite_column]\n",
    "    df_merged.to_csv(merged_path, index=False)\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_columns(r'B:\\Publications\\FAIR OER\\02-Data\\01-Processed\\courses_clean_en_norm.csv', [0, 2, 3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = [\"sentence-transformers/all-mpnet-base-v2\", \"sentence-transformers/all-MiniLM-L6-v2\", \"sentence-transformers/all-MiniLM-L12-v2\"]\n",
    "\n",
    "data = pd.read_csv(r'B:\\Publications\\FAIR OER\\02-Data\\01-Processed\\courses_clean_en_norm_merged.csv')\n",
    "data['course'].fillna('', inplace=True)\n",
    "data['prerequisites'].fillna('', inplace=True)\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] \n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "for checkpoint in checkpoints:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    model = AutoModel.from_pretrained(checkpoint)\n",
    "    model.eval()\n",
    "    course = data['course'].tolist()\n",
    "    prerequisite = data['prerequisites'].tolist()\n",
    "    \n",
    "    course_tokens = []\n",
    "    prerequisite_tokens = []\n",
    "    for i in range(len(course)):\n",
    "        course_tokens.append(tokenizer(course[i], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\"))\n",
    "        prerequisite_tokens.append(tokenizer(prerequisite[i], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\"))\n",
    "    \n",
    "    course_embeddings = []\n",
    "    prerequisite_embeddings = []\n",
    "    for input in course_tokens:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input)\n",
    "            course_embeddings.append(outputs)\n",
    "    for input in prerequisite_tokens:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input)\n",
    "            prerequisite_embeddings.append(outputs)\n",
    "    \n",
    "    course_pooled_embeddings = []\n",
    "    prerequisite_pooled_embeddings = []\n",
    "    for i, embeddings in enumerate(course_embeddings):\n",
    "        mean_pooled = mean_pooling(embeddings, course_tokens[i]['attention_mask'])\n",
    "        course_pooled_embeddings.append(F.normalize(mean_pooled, p=2, dim=1))\n",
    "    for i, embeddings in enumerate(prerequisite_embeddings):\n",
    "        mean_pooled = mean_pooling(embeddings, prerequisite_tokens[i]['attention_mask'])\n",
    "        prerequisite_pooled_embeddings.append(F.normalize(mean_pooled, p=2, dim=1))\n",
    "    \n",
    "    similarity_scores = np.zeros((len(prerequisite_pooled_embeddings), len(course_pooled_embeddings)))\n",
    "    for i, p in enumerate(prerequisite_pooled_embeddings):\n",
    "        for j, c in enumerate(course_pooled_embeddings):\n",
    "            p_expanded = p.squeeze()\n",
    "            c_expanded = c.squeeze()\n",
    "            similarity = F.cosine_similarity(p_expanded, c_expanded, dim=0)\n",
    "            similarity_scores[i][j] = similarity.item()\n",
    "            similarity_df = pd.DataFrame(similarity_scores)\n",
    "            similarity_df.to_csv(rf'B:\\Publications\\FAIR OER\\02-Data\\01-Processed\\similarity_{checkpoint.replace(\"/\", \"-\")}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df = pd.DataFrame(similarity_scores)\n",
    "similarity_df.to_csv(r\"B:\\Workspace\\FAIR OER\\Codes\\final\\similarity_title.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"B:\\Workspace\\FAIR OER\\Codes\\final\\similarity_title.csv\")\n",
    "courses=pd.read_csv(r\"B:\\Workspace\\FAIR OER\\Codes\\final\\courses_clean_en_norm.csv\")\n",
    "courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where(df >= 0.7)\n",
    "np.fill_diagonal(df.values, np.nan)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    for j in range(len(df.columns)):\n",
    "        if df.iloc[i,j] > 0:\n",
    "            df.iloc[i,j] = courses.iloc[j,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat([courses, df], axis=1)\n",
    "final.to_csv(r\"B:\\Workspace\\FAIR OER\\Codes\\final\\final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Nd7b04782b31e4ebc9b97c784b77477db (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"B:\\LMS\\ETKG\\Data\\ETKG-v2.csv\")\n",
    "\n",
    "etkg = Namespace('http://etkg.nfdi4earth.de/')\n",
    "schema = Namespace('http://schema.org/')\n",
    "lrmi = Namespace('http://purl.org/dcx/lrmi-terms/')\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "g.bind('etkg', etkg)\n",
    "g.bind('xsd', XSD)\n",
    "g.bind('dcterms', DCTERMS)\n",
    "g.bind('schema', schema)\n",
    "g.bind('lrmi', lrmi)\n",
    "\n",
    "def create_uri_component(code):\n",
    "    return re.sub(r'\\s+', '-', code)\n",
    "\n",
    "def add_row_to_graph(g, row):\n",
    "    course_uri= create_uri_component(str(row['code']))\n",
    "    course_uri = etkg[course_uri]\n",
    "\n",
    "    g.add((course_uri, RDF.type, schema.LearningResource))\n",
    "    g.add((course_uri, DCTERMS.title, Literal(str(row['name']), datatype=XSD.string)))\n",
    "    g.add((course_uri, lrmi.learningObjective, Literal(str(row['description']), datatype=XSD.string)))\n",
    "    g.add((course_uri, schema.educationalLevel, Literal(str(row['cognitive domain']), datatype=XSD.string)))\n",
    "    g.add((course_uri, schema.educationalLevel, Literal(str(row['difficulty']), datatype=XSD.string)))\n",
    "    if pd.notnull(row['prerequisite1']):\n",
    "        g.add((course_uri, schema.competencyRequired, URIRef(etkg[create_uri_component(str(row['prerequisite1']))])))\n",
    "    if pd.notnull(row['prerequisite2']):\n",
    "        g.add((course_uri, schema.competencyRequired, URIRef(etkg[create_uri_component(str(row['prerequisite2']))])))\n",
    "    if pd.notnull(row['prerequisite3']):\n",
    "        g.add((course_uri, schema.competencyRequired, URIRef(etkg[create_uri_component(str(row['prerequisite3']))])))\n",
    "    if pd.notnull(row['prerequisite4']):\n",
    "        g.add((course_uri, schema.competencyRequired, URIRef(etkg[create_uri_component(str(row['prerequisite4']))])))\n",
    "    if pd.notnull(row['prerequisite5']):\n",
    "        g.add((course_uri, schema.competencyRequired, URIRef(etkg[create_uri_component(str(row['prerequisite5']))])))\n",
    "    if pd.notnull(row['prerequisite6']):\n",
    "        g.add((course_uri, schema.competencyRequired, URIRef(etkg[create_uri_component(str(row['prerequisite6']))])))\n",
    "    if pd.notnull(row['learning resource1']):\n",
    "        g.add((course_uri, DCTERMS.source, URIRef(str(row['learning resource1']))))\n",
    "    if pd.notnull(row['learning resource2']):\n",
    "        g.add((course_uri, DCTERMS.source, URIRef(str(row['learning resource2']))))\n",
    "    if pd.notnull(row['learning resource3']):\n",
    "        g.add((course_uri, DCTERMS.source, URIRef(str(row['learning resource3']))))\n",
    "    if pd.notnull(row['learning resource4']):\n",
    "        g.add((course_uri, DCTERMS.source, URIRef(str(row['learning resource4']))))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    add_row_to_graph(g, row)\n",
    "\n",
    "\n",
    "g.serialize(destination=r\"B:\\LMS\\ETKG\\Data\\ETKG-v2.ttl\", format='turtle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of triples: 397\n"
     ]
    }
   ],
   "source": [
    "g = Graph()\n",
    "g.parse(r\"B:\\LMS\\ETKG\\Codes\\ETKG-test.ttl\", format=\"ttl\")\n",
    "\n",
    "print(f\"Number of triples: {len(g)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node added: http://etkg.nfdi4earth.de/EL216 with title Fundamentals of pandas\n",
      "Node added: http://etkg.nfdi4earth.de/EL242 with title Analyze raster data in Python\n",
      "Node added: http://etkg.nfdi4earth.de/EL190 with title Introduction to open reproducible science\n",
      "Node added: http://etkg.nfdi4earth.de/TUM4 with title Fundamentals of spatial data mining and machine learning \n",
      "Node added: http://etkg.nfdi4earth.de/TUM2 with title Fundamentals of big spatial data \n",
      "Node added: http://etkg.nfdi4earth.de/EL265 with title Introduction to Light Detection and Ranging (Lidar)data\n",
      "Node added: http://etkg.nfdi4earth.de/DLS1GD with title Introduction to spatial data\n",
      "Node added: http://etkg.nfdi4earth.de/EL290 with title Work with raster data in Python\n",
      "Node added: http://etkg.nfdi4earth.de/EL127 with title Introduction to using Git for scientific projects\n",
      "Node added: http://etkg.nfdi4earth.de/EL200 with title Introduction to Python\n",
      "Node added: http://etkg.nfdi4earth.de/DLS1DM3-2 with title Introduction to raster data\n",
      "Node added: http://etkg.nfdi4earth.de/EL357 with title Fundamentals of geopandas\n",
      "Node added: http://etkg.nfdi4earth.de/EL205 with title Introduction to Python packages for Earth data science\n",
      "Node added: http://etkg.nfdi4earth.de/EL273 with title Introduction to hierarchical data formats\n",
      "Node added: http://etkg.nfdi4earth.de/TUM3 with title Evaluate remote sensing data in Python\n",
      "Node added: http://etkg.nfdi4earth.de/EL188 with title Introduction to Bash\n",
      "Node added: http://etkg.nfdi4earth.de/EL208 with title Introduction to science project management with Python\n",
      "Node added: http://etkg.nfdi4earth.de/EL220 with title Fundamentals of writing clean, efficient Python code\n",
      "Node added: http://etkg.nfdi4earth.de/EL319 with title Introduction to remote sensing data\n",
      "Node added: http://etkg.nfdi4earth.de/DLS4PS1-3-2 with title Analyze LiDAR data in Python\n",
      "Node added: http://etkg.nfdi4earth.de/DLS4AM with title Evaluate spatial data in Python\n",
      "Node added: http://etkg.nfdi4earth.de/EL275 with title Work with NetCDF data in Python\n",
      "Node added: http://etkg.nfdi4earth.de/EL312 with title Work with vector data in Python\n",
      "Node added: http://etkg.nfdi4earth.de/EL175 with title Work with Git for collaboration\n",
      "Node added: http://etkg.nfdi4earth.de/DLS3PS1-3-2 with title Fundamentals of GDAL\n",
      "Node added: http://etkg.nfdi4earth.de/EL171 with title Introduction to text file formats for Earth data science\n",
      "Node added: http://etkg.nfdi4earth.de/DLS1DM4 with title Introduction to vector data\n",
      "Node added: http://etkg.nfdi4earth.de/TUM1 with title Fundamentals of mathematics for Earth data science\n",
      "Node added: http://etkg.nfdi4earth.de/EL346 with title Introduction to time series\n",
      "Node added: http://etkg.nfdi4earth.de/EL270 with title Earth data science workflows\n",
      "Node added: http://etkg.nfdi4earth.de/EL370 with title Setting up a python Earth data analysis environment\n",
      "Node added: http://etkg.nfdi4earth.de/EL298 with title Work with APIs for accessing spatial data\n",
      "Node added: http://etkg.nfdi4earth.de/DLS4GD2-2 with title Analyze remote sensing data in Python\n",
      "Node added: http://etkg.nfdi4earth.de/EL252 with title Fundamentals of matplotlib\n",
      "Node added: http://etkg.nfdi4earth.de/EL276 with title Introduction to the HDF4 data\n",
      "Node added: http://etkg.nfdi4earth.de/EL167 with title Fundamentals of spatial data formats for Earth data science\n",
      "Node added: http://etkg.nfdi4earth.de/DLS1PS3-6 with title Introduction to the NetCDF data\n",
      "Node added: http://etkg.nfdi4earth.de/DLS1GD1-2 with title Introduction to coordinate reference systems\n",
      "Node added: http://etkg.nfdi4earth.de/EL212 with title Fundamentals of numpy\n",
      "Node added: http://etkg.nfdi4earth.de/DLS3GIST with title Introduction to Python packages for data science\n",
      "Node added: http://etkg.nfdi4earth.de/DLS5CV4-5 with title Fundamentals of folium\n",
      "Node added: http://etkg.nfdi4earth.de/EL283 with title Fundamentals of raster data in Python\n",
      "Node added: http://etkg.nfdi4earth.de/EL358 with title Fundamentals of rasterio\n",
      "Node added: http://etkg.nfdi4earth.de/EL193 with title Introduction to Jupyter for Python\n",
      "Node added: http://etkg.nfdi4earth.de/EL247 with title Analyze vector data in Python\n",
      "Node added: http://etkg.nfdi4earth.de/EL308 with title Fundamentals of vector data in Python\n",
      "Node added: http://etkg.nfdi4earth.de/DLSGIST with title Introduction to the science of Geographic Information Systems\n",
      "Node added: http://etkg.nfdi4earth.de/EL181 with title Fundamentals of version control\n",
      "Node added: http://etkg.nfdi4earth.de/EL334 with title Evaluate lidar data in Python\n",
      "Edge added: http://etkg.nfdi4earth.de/EL346 -> http://etkg.nfdi4earth.de/EL370\n",
      "Edge added: http://etkg.nfdi4earth.de/EL171 -> http://etkg.nfdi4earth.de/EL193\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS1PS3-6 -> http://etkg.nfdi4earth.de/EL273\n",
      "Edge added: http://etkg.nfdi4earth.de/EL188 -> http://etkg.nfdi4earth.de/EL190\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS4AM -> http://etkg.nfdi4earth.de/EL216\n",
      "Edge added: http://etkg.nfdi4earth.de/EL205 -> http://etkg.nfdi4earth.de/DLS3GIST\n",
      "Edge added: http://etkg.nfdi4earth.de/EL358 -> http://etkg.nfdi4earth.de/EL283\n",
      "Edge added: http://etkg.nfdi4earth.de/EL270 -> http://etkg.nfdi4earth.de/EL220\n",
      "Edge added: http://etkg.nfdi4earth.de/EL283 -> http://etkg.nfdi4earth.de/EL200\n",
      "Edge added: http://etkg.nfdi4earth.de/EL346 -> http://etkg.nfdi4earth.de/EL205\n",
      "Edge added: http://etkg.nfdi4earth.de/EL265 -> http://etkg.nfdi4earth.de/DLS1GD\n",
      "Edge added: http://etkg.nfdi4earth.de/EL200 -> http://etkg.nfdi4earth.de/EL193\n",
      "Edge added: http://etkg.nfdi4earth.de/EL242 -> http://etkg.nfdi4earth.de/EL252\n",
      "Edge added: http://etkg.nfdi4earth.de/EL247 -> http://etkg.nfdi4earth.de/DLS3GIST\n",
      "Edge added: http://etkg.nfdi4earth.de/EL208 -> http://etkg.nfdi4earth.de/EL370\n",
      "Edge added: http://etkg.nfdi4earth.de/EL188 -> http://etkg.nfdi4earth.de/EL370\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS1GD -> http://etkg.nfdi4earth.de/DLSGIST\n",
      "Edge added: http://etkg.nfdi4earth.de/EL357 -> http://etkg.nfdi4earth.de/EL308\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS4AM -> http://etkg.nfdi4earth.de/EL312\n",
      "Edge added: http://etkg.nfdi4earth.de/EL312 -> http://etkg.nfdi4earth.de/EL357\n",
      "Edge added: http://etkg.nfdi4earth.de/EL273 -> http://etkg.nfdi4earth.de/EL200\n",
      "Edge added: http://etkg.nfdi4earth.de/EL283 -> http://etkg.nfdi4earth.de/EL167\n",
      "Edge added: http://etkg.nfdi4earth.de/EL193 -> http://etkg.nfdi4earth.de/EL370\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS1DM4 -> http://etkg.nfdi4earth.de/DLS1GD\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS3GIST -> http://etkg.nfdi4earth.de/EL200\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS4AM -> http://etkg.nfdi4earth.de/EL290\n",
      "Edge added: http://etkg.nfdi4earth.de/EL216 -> http://etkg.nfdi4earth.de/EL212\n",
      "Edge added: http://etkg.nfdi4earth.de/EL175 -> http://etkg.nfdi4earth.de/EL370\n",
      "Edge added: http://etkg.nfdi4earth.de/EL308 -> http://etkg.nfdi4earth.de/EL200\n",
      "Edge added: http://etkg.nfdi4earth.de/TUM4 -> http://etkg.nfdi4earth.de/TUM2\n",
      "Edge added: http://etkg.nfdi4earth.de/EL247 -> http://etkg.nfdi4earth.de/EL252\n",
      "Edge added: http://etkg.nfdi4earth.de/EL220 -> http://etkg.nfdi4earth.de/EL216\n",
      "Edge added: http://etkg.nfdi4earth.de/EL205 -> http://etkg.nfdi4earth.de/EL167\n",
      "Edge added: http://etkg.nfdi4earth.de/EL290 -> http://etkg.nfdi4earth.de/EL283\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS3PS1-3-2 -> http://etkg.nfdi4earth.de/EL358\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS4PS1-3-2 -> http://etkg.nfdi4earth.de/EL205\n",
      "Edge added: http://etkg.nfdi4earth.de/EL275 -> http://etkg.nfdi4earth.de/EL370\n",
      "Edge added: http://etkg.nfdi4earth.de/TUM3 -> http://etkg.nfdi4earth.de/DLS4GD2-2\n",
      "Edge added: http://etkg.nfdi4earth.de/EL167 -> http://etkg.nfdi4earth.de/EL265\n",
      "Edge added: http://etkg.nfdi4earth.de/TUM2 -> http://etkg.nfdi4earth.de/EL167\n",
      "Edge added: http://etkg.nfdi4earth.de/EL200 -> http://etkg.nfdi4earth.de/EL370\n",
      "Edge added: http://etkg.nfdi4earth.de/EL308 -> http://etkg.nfdi4earth.de/EL167\n",
      "Edge added: http://etkg.nfdi4earth.de/EL242 -> http://etkg.nfdi4earth.de/DLS3GIST\n",
      "Edge added: http://etkg.nfdi4earth.de/EL319 -> http://etkg.nfdi4earth.de/EL167\n",
      "Edge added: http://etkg.nfdi4earth.de/EL220 -> http://etkg.nfdi4earth.de/EL208\n",
      "Edge added: http://etkg.nfdi4earth.de/EL298 -> http://etkg.nfdi4earth.de/EL167\n",
      "Edge added: http://etkg.nfdi4earth.de/EL181 -> http://etkg.nfdi4earth.de/EL127\n",
      "Edge added: http://etkg.nfdi4earth.de/EL167 -> http://etkg.nfdi4earth.de/DLS1GD1-2\n",
      "Edge added: http://etkg.nfdi4earth.de/TUM2 -> http://etkg.nfdi4earth.de/EL220\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS1DM3-2 -> http://etkg.nfdi4earth.de/DLS1GD\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS4GD2-2 -> http://etkg.nfdi4earth.de/EL358\n",
      "Edge added: http://etkg.nfdi4earth.de/EL216 -> http://etkg.nfdi4earth.de/EL205\n",
      "Edge added: http://etkg.nfdi4earth.de/EL208 -> http://etkg.nfdi4earth.de/EL193\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS4AM -> http://etkg.nfdi4earth.de/EL357\n",
      "Edge added: http://etkg.nfdi4earth.de/EL212 -> http://etkg.nfdi4earth.de/EL205\n",
      "Edge added: http://etkg.nfdi4earth.de/EL334 -> http://etkg.nfdi4earth.de/DLS4PS1-3-2\n",
      "Edge added: http://etkg.nfdi4earth.de/EL242 -> http://etkg.nfdi4earth.de/EL290\n",
      "Edge added: http://etkg.nfdi4earth.de/EL346 -> http://etkg.nfdi4earth.de/EL216\n",
      "Edge added: http://etkg.nfdi4earth.de/EL290 -> http://etkg.nfdi4earth.de/EL358\n",
      "Edge added: http://etkg.nfdi4earth.de/EL275 -> http://etkg.nfdi4earth.de/DLS1PS3-6\n",
      "Edge added: http://etkg.nfdi4earth.de/EL175 -> http://etkg.nfdi4earth.de/EL181\n",
      "Edge added: http://etkg.nfdi4earth.de/EL312 -> http://etkg.nfdi4earth.de/EL308\n",
      "Edge added: http://etkg.nfdi4earth.de/EL298 -> http://etkg.nfdi4earth.de/EL220\n",
      "Edge added: http://etkg.nfdi4earth.de/EL358 -> http://etkg.nfdi4earth.de/EL252\n",
      "Edge added: http://etkg.nfdi4earth.de/EL346 -> http://etkg.nfdi4earth.de/EL200\n",
      "Edge added: http://etkg.nfdi4earth.de/EL181 -> http://etkg.nfdi4earth.de/EL370\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS5CV4-5 -> http://etkg.nfdi4earth.de/EL357\n",
      "Edge added: http://etkg.nfdi4earth.de/EL247 -> http://etkg.nfdi4earth.de/EL312\n",
      "Edge added: http://etkg.nfdi4earth.de/EL167 -> http://etkg.nfdi4earth.de/EL171\n",
      "Edge added: http://etkg.nfdi4earth.de/EL252 -> http://etkg.nfdi4earth.de/EL205\n",
      "Edge added: http://etkg.nfdi4earth.de/EL220 -> http://etkg.nfdi4earth.de/EL200\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS4GD2-2 -> http://etkg.nfdi4earth.de/EL319\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS4AM -> http://etkg.nfdi4earth.de/EL252\n",
      "Edge added: http://etkg.nfdi4earth.de/TUM3 -> http://etkg.nfdi4earth.de/EL200\n",
      "Edge added: http://etkg.nfdi4earth.de/EL298 -> http://etkg.nfdi4earth.de/EL357\n",
      "Edge added: http://etkg.nfdi4earth.de/EL242 -> http://etkg.nfdi4earth.de/EL370\n",
      "Edge added: http://etkg.nfdi4earth.de/EL220 -> http://etkg.nfdi4earth.de/EL212\n",
      "Edge added: http://etkg.nfdi4earth.de/EL334 -> http://etkg.nfdi4earth.de/EL265\n",
      "Edge added: http://etkg.nfdi4earth.de/EL270 -> http://etkg.nfdi4earth.de/EL205\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS1GD1-2 -> http://etkg.nfdi4earth.de/DLS1DM4\n",
      "Edge added: http://etkg.nfdi4earth.de/EL167 -> http://etkg.nfdi4earth.de/DLS1DM4\n",
      "Edge added: http://etkg.nfdi4earth.de/EL276 -> http://etkg.nfdi4earth.de/EL273\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS3GIST -> http://etkg.nfdi4earth.de/EL208\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS4PS1-3-2 -> http://etkg.nfdi4earth.de/EL265\n",
      "Edge added: http://etkg.nfdi4earth.de/EL247 -> http://etkg.nfdi4earth.de/EL370\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS1GD1-2 -> http://etkg.nfdi4earth.de/DLS1DM3-2\n",
      "Edge added: http://etkg.nfdi4earth.de/EL167 -> http://etkg.nfdi4earth.de/DLS1DM3-2\n",
      "Edge added: http://etkg.nfdi4earth.de/EL357 -> http://etkg.nfdi4earth.de/EL252\n",
      "Edge added: http://etkg.nfdi4earth.de/TUM3 -> http://etkg.nfdi4earth.de/TUM1\n",
      "Edge added: http://etkg.nfdi4earth.de/DLS3PS1-3-2 -> http://etkg.nfdi4earth.de/EL265\n"
     ]
    }
   ],
   "source": [
    "dcterms = rdflib.Namespace(\"http://purl.org/dc/terms/\")\n",
    "schema = rdflib.Namespace(\"http://schema.org/\")\n",
    "etkg = rdflib.Namespace(\"http://etkg.nfdi4earth.de/\")\n",
    "\n",
    "nodes = {}\n",
    "edges = []\n",
    "\n",
    "for s, p, o in g:\n",
    "    if p == dcterms.title:\n",
    "        nodes[str(s)] = str(o)\n",
    "        print(f\"Node added: {s} with title {o}\")\n",
    "\n",
    "for s, p, o in g:\n",
    "    if p == schema.competencyRequired:\n",
    "        edges.append((str(s), str(o)))\n",
    "        print(f\"Edge added: {s} -> {o}\")\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for node, title in nodes.items():\n",
    "    G.add_node(node, title=title)\n",
    "\n",
    "for s, o in edges:\n",
    "    if s in nodes and o in nodes:\n",
    "        G.add_edge(s, o)\n",
    "\n",
    "node_titles = {node: data['title'] for node, data in G.nodes(data=True)}\n",
    "\n",
    "net = Network(notebook=True, cdn_resources='in_line')\n",
    "\n",
    "net.toggle_physics(True)\n",
    "net.show_buttons(filter_=['physics'])\n",
    "\n",
    "for node, title in node_titles.items():\n",
    "    net.add_node(node, label=title, title=title, shape='dot', size=10)\n",
    "\n",
    "for s, o in edges:\n",
    "    if s in node_titles and o in node_titles:\n",
    "        net.add_edge(s, o)\n",
    "\n",
    "net.save_graph(\"graph-test.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(height='100vh', width='100%')\n",
    "\n",
    "for node, title in node_titles.items():\n",
    "    wrapped_title = textwrap.fill(title, width=19)  \n",
    "    net.add_node(node, label=wrapped_title, title=wrapped_title, color={'background': '#bfd6de', 'border': '#003f60'}, borderWidth=2, font={'color': '#003f60', 'size': 11}, size=37, shape='circle')\n",
    "\n",
    "for s, o in edges:\n",
    "    if s in node_titles and o in node_titles:\n",
    "        net.add_edge(s, o, color='#003f60', width=2, title='Prerequisite', label='Prerequisite', arrows='to', font={'align': 'top'})\n",
    "\n",
    "net.toggle_drag_nodes(True)\n",
    "net.toggle_physics(True)\n",
    "net.show_buttons(filter_=['physics'])\n",
    "net.save_graph(\"graph-test.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_info = {}\n",
    "\n",
    "for s, p, o in g:\n",
    "    s_str, p_str, o_str = str(s), str(p), str(o)\n",
    "    if p_str in {\n",
    "        'http://purl.org/dcx/lrmi-terms/learningObjective',\n",
    "        'http://purl.org/dc/terms/source',\n",
    "        'http://schema.org/competencyRequired',\n",
    "        'http://schema.org/educationalLevel'\n",
    "    }:\n",
    "        if s_str not in additional_info:\n",
    "            additional_info[s_str] = {\n",
    "                'learning_objectives': [],\n",
    "                'sources': [],\n",
    "                'prerequisites': [],\n",
    "                'level': []\n",
    "            }\n",
    "        if p_str == 'http://purl.org/dcx/lrmi-terms/learningObjective':\n",
    "            additional_info[s_str]['learning_objectives'].append(o_str)\n",
    "        elif p_str == 'http://purl.org/dc/terms/source':\n",
    "            additional_info[s_str]['sources'].append(o_str)\n",
    "        elif p_str == 'http://schema.org/competencyRequired':\n",
    "            additional_info[s_str]['prerequisites'].append(o_str)\n",
    "        elif p_str == 'http://schema.org/educationalLevel':\n",
    "            additional_info[s_str]['level'].append(o_str)\n",
    "\n",
    "for node in additional_info:\n",
    "    for key in additional_info[node]:\n",
    "        additional_info[node][key] = ', '.join(additional_info[node][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(height='100vh', width='100%')\n",
    "\n",
    "for node, title in nodes.items():\n",
    "    wrapped_title = textwrap.fill(title, width=19)\n",
    "    node_info = additional_info.get(node, {\n",
    "        'learning_objectives': 'N/A',\n",
    "        'sources': 'N/A',\n",
    "        'prerequisites': 'N/A',\n",
    "        'level': 'N/A'\n",
    "    })\n",
    "    info_html = f\"\"\"\n",
    "    Title: \n",
    "    {title}\n",
    "    Learning Objectives:\n",
    "    {node_info['learning_objectives']}\n",
    "    Sources:\n",
    "    {node_info['sources']}\n",
    "    Prerequisites: \n",
    "    {node_info['prerequisites']}\n",
    "    Level: \n",
    "    {node_info['level']}\n",
    "    \"\"\"\n",
    "    net.add_node(node, label=wrapped_title, title=info_html, color={'background': '#bfd6de', 'border': '#003f60'}, borderWidth=2, font={'color': '#003f60', 'size': 11}, size=37, shape='circle')\n",
    "\n",
    "for s, o in edges:\n",
    "    if s in nodes and o in nodes:\n",
    "        net.add_edge(s, o, color='#003f60', width=2, title='Prerequisite', label='Prerequisite', arrows='to', font={'align': 'top'})\n",
    "\n",
    "net.toggle_physics(True)\n",
    "net.show_buttons(filter_=['physics'])\n",
    "net.save_graph(\"graph-test.html\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
